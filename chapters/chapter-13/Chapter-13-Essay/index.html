
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A comprehensive guide to the foundations of quantum computing">
      
      
        <meta name="author" content="Big Book of Computing">
      
      
        <link rel="canonical" href="https://bigbookofcomputing.github.io/chapters/chapter-13/Chapter-13-Essay/">
      
      
        <link rel="prev" href="../../chapter-12/Chapter-12-Essay/">
      
      
        <link rel="next" href="../../chapter-14/Chapter-14-Essay/">
      
      
        
      
      
      <link rel="icon" href="../../../assets/logo.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Chapter-13 Quantum Reinforcement Learning - Big Book of Computing | Quantum Computing</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    
      <link rel="stylesheet" href="../../../static/styles.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-ECS7B3X8JM"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-ECS7B3X8JM",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-ECS7B3X8JM",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script>
  

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-13-quantum-reinforcement-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Big Book of Computing | Quantum Computing" class="md-header__button md-logo" aria-label="Big Book of Computing | Quantum Computing" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Big Book of Computing | Quantum Computing
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter-13 Quantum Reinforcement Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/bigbookofcomputing/quantum" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../contents/" class="md-tabs__link">
        
  
  
    
  
  Contents

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../introduction/" class="md-tabs__link">
        
  
  
    
  
  Introduction

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../chapter-1/Chapter-1-Essay/" class="md-tabs__link">
          
  
  
    
  
  Chapters

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter-1/Chapter-1-Workbook/" class="md-tabs__link">
          
  
  
    
  
  WorkBooks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter-1/Chapter-1-Codebook/" class="md-tabs__link">
          
  
  
    
  
  CodeBooks

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Big Book of Computing | Quantum Computing" class="md-nav__button md-logo" aria-label="Big Book of Computing | Quantum Computing" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    Big Book of Computing | Quantum Computing
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/bigbookofcomputing/quantum" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contents
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Chapters
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Chapters
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-1/Chapter-1-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-1 Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-2/Chapter-2-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-2 State and Operators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-3/Chapter-3-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-3 Gate and Circuits
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-4/Chapter-4-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-4 Quantum Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-5/Chapter-5-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-5 Quantum Fourier Transform
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-6/Chapter-6-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-6 Variational Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-7/Chapter-7-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-7 Quantum Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-8/Chapter-8-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-8 Quantum Machine Learning and Optimization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-9/Chapter-9-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-9 Encoding Classical Data into Quantum States
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-10/Chapter-10-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-10 Variational Quantum Classifiers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-11/Chapter-11-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-11 Supervised Quantum Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-12/Chapter-12-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-12 Unsupervised Quantum Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Chapter-13 Quantum Reinforcement Learning
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-13 Quantum Reinforcement Learning
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-outline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Outline
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#131-qrl-basics" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.1 QRL Basics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.1 QRL Basics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-classical-rl-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Classical RL Framework
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-qrl-extension-and-policy-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        The QRL Extension and Policy Representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-hybrid-nature" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Hybrid Nature
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#132-quantum-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.2 Quantum Policy Gradient Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.2 Quantum Policy Gradient Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policy-representation-and-action-sampling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy Representation and Action Sampling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-quantum-policy-gradient-update" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Quantum Policy Gradient Update
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-calculation-and-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gradient Calculation and Efficiency
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#133-quantum-value-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.3 Quantum Value Iteration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.3 Quantum Value Iteration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#value-function-approximation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Value Function Approximation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-quantum-bellman-equation-and-temporal-difference-error" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Quantum Bellman Equation and Temporal-Difference Error
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantum-deep-q-networks-vq-dqn" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quantum Deep Q-Networks (VQ-DQN)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#134-quantum-exploration-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.4 Quantum Exploration Strategies
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.4 Quantum Exploration Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#non-classical-exploration-mechanisms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Non-Classical Exploration Mechanisms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coherent-exploration-vs-epsilon-greedy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Coherent Exploration vs. \(\epsilon\)-Greedy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#135-quantum-agent-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.5 Quantum Agent Architectures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.5 Quantum Agent Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-hybrid-components" class="md-nav__link">
    <span class="md-ellipsis">
      
        Core Hybrid Components
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-quantum-environment" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Quantum Environment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Advanced Architectures
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-quantum-reinforcement-learning-paradigms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary: Quantum Reinforcement Learning Paradigms
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Summary: Quantum Reinforcement Learning Paradigms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#i-core-paradigms-and-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        I. Core Paradigms and Objectives
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ii-mechanism-and-resource-utilization" class="md-nav__link">
    <span class="md-ellipsis">
      
        II. Mechanism and Resource Utilization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#iii-exploration-and-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        III. Exploration and Architecture
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-14/Chapter-14-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-14 QUBO Family of Problems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-15/Chapter-15-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-15 Quantum Data Pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-16/Chapter-16-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-16 Quantum Simulation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-17/Chapter-17-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-17 Quantum Chemistry
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-18/Chapter-18-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-18 Quantum Finance
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-19/Chapter-19-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-19 Quantum Hardware
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-20/Chapter-20-Essay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-20 Quantum Error Correction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    WorkBooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    WorkBooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-1/Chapter-1-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-1 Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-2/Chapter-2-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-2 State and Operators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-3/Chapter-3-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-3 Gate and Circuits
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-4/Chapter-4-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-4 Quantum Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-5/Chapter-5-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-5 Quantum Fourier Transform
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-6/Chapter-6-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-6 Variational Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-7/Chapter-7-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-7 Quantum Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-8/Chapter-8-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-8 Quantum Machine Learning and Optimization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-9/Chapter-9-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-9 Encoding Classical Data into Quantum States
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-10/Chapter-10-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-10 Variational Quantum Classifiers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-11/Chapter-11-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-11 Supervised Quantum Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-12/Chapter-12-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-12 Unsupervised Quantum Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter-13-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-13 Quantum Reinforcement Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-14/Chapter-14-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-14 QUBO Family of Problems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-15/Chapter-15-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-15 Quantum Data Pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-16/Chapter-16-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-16 Quantum Simulation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-17/Chapter-17-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-17 Quantum Chemistry
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-18/Chapter-18-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-18 Quantum Finance
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-19/Chapter-19-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-19 Quantum Hardware
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-20/Chapter-20-Workbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-20 Quantum Error Correction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    CodeBooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    CodeBooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-1/Chapter-1-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-1 Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-2/Chapter-2-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-2 State and Operators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-3/Chapter-3-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-3 Gate and Circuits
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-4/Chapter-4-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-4 Quantum Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-5/Chapter-5-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-5 Quantum Fourier Transform
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-6/Chapter-6-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-6 Variational Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-7/Chapter-7-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-7 Quantum Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-8/Chapter-8-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-8 Quantum Machine Learning and Optimization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-9/Chapter-9-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-9 Encoding Classical Data into Quantum States
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-10/Chapter-10-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-10 Variational Quantum Classifiers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-11/Chapter-11-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-11 Supervised Quantum Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-12/Chapter-12-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-12 Unsupervised Quantum Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter-13-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-13 Quantum Reinforcement Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-14/Chapter-14-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-14 QUBO Family of Problems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-15/Chapter-15-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-15 Quantum Data Pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-16/Chapter-16-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-16 Quantum Simulation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-17/Chapter-17-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-17 Quantum Chemistry
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-18/Chapter-18-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-18 Quantum Finance
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-19/Chapter-19-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-19 Quantum Hardware
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter-20/Chapter-20-Codebook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter-20 Quantum Error Correction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-outline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Outline
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#131-qrl-basics" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.1 QRL Basics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.1 QRL Basics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-classical-rl-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Classical RL Framework
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-qrl-extension-and-policy-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        The QRL Extension and Policy Representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-hybrid-nature" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Hybrid Nature
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#132-quantum-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.2 Quantum Policy Gradient Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.2 Quantum Policy Gradient Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policy-representation-and-action-sampling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy Representation and Action Sampling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-quantum-policy-gradient-update" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Quantum Policy Gradient Update
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-calculation-and-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gradient Calculation and Efficiency
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#133-quantum-value-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.3 Quantum Value Iteration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.3 Quantum Value Iteration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#value-function-approximation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Value Function Approximation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-quantum-bellman-equation-and-temporal-difference-error" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Quantum Bellman Equation and Temporal-Difference Error
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantum-deep-q-networks-vq-dqn" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quantum Deep Q-Networks (VQ-DQN)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#134-quantum-exploration-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.4 Quantum Exploration Strategies
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.4 Quantum Exploration Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#non-classical-exploration-mechanisms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Non-Classical Exploration Mechanisms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coherent-exploration-vs-epsilon-greedy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Coherent Exploration vs. \(\epsilon\)-Greedy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#135-quantum-agent-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      
        13.5 Quantum Agent Architectures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="13.5 Quantum Agent Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-hybrid-components" class="md-nav__link">
    <span class="md-ellipsis">
      
        Core Hybrid Components
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-quantum-environment" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Quantum Environment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Advanced Architectures
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-quantum-reinforcement-learning-paradigms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary: Quantum Reinforcement Learning Paradigms
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Summary: Quantum Reinforcement Learning Paradigms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#i-core-paradigms-and-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        I. Core Paradigms and Objectives
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ii-mechanism-and-resource-utilization" class="md-nav__link">
    <span class="md-ellipsis">
      
        II. Mechanism and Resource Utilization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#iii-exploration-and-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        III. Exploration and Architecture
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/bigbookofcomputing/quantum/edit/master/docs/chapters/chapter-13/Chapter-13-Essay.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/bigbookofcomputing/quantum/raw/master/docs/chapters/chapter-13/Chapter-13-Essay.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="chapter-13-quantum-reinforcement-learning"><strong>Chapter 13: Quantum Reinforcement Learning</strong></h1>
<hr />
<h2 id="introduction"><strong>Introduction</strong></h2>
<p>Reinforcement Learning (RL) represents one of the most powerful paradigms in machine learning, enabling agents to learn optimal decision-making strategies through trial-and-error interaction with an environment. Classical RL has achieved remarkable success in domains ranging from game playing to robotics. However, as state spaces grow exponentially and environments become more complex, classical function approximation methods face severe computational bottlenecks in policy optimization and value function learning.</p>
<p>Quantum Reinforcement Learning (QRL) emerges as a hybrid computational paradigm that integrates quantum computing resourcessuperposition, entanglement, and coherent evolutioninto the RL framework. By representing policies and value functions as Parameterized Quantum Circuits (PQCs), QRL aims to exploit the exponential expressivity of quantum Hilbert space for richer function approximation. Moreover, quantum exploration strategies such as quantum random walks and amplitude amplification offer fundamentally different mechanisms for navigating state-action spaces compared to classical stochastic exploration [1, 2].</p>
<p>This chapter examines the core QRL paradigms: quantum policy gradient methods, quantum value iteration, coherent exploration strategies, and hybrid agent architectures. Understanding these quantum extensions to RL reveals how quantum computation may provide advantage in learning optimal behavior in complex, high-dimensional environments.</p>
<hr />
<h2 id="chapter-outline"><strong>Chapter Outline</strong></h2>
<table>
<thead>
<tr>
<th><strong>Sec.</strong></th>
<th><strong>Title</strong></th>
<th><strong>Core Ideas &amp; Examples</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>13.1</strong></td>
<td><strong>QRL Basics</strong></td>
<td>MDP framework: state <span class="arithmatex">\(s_t\)</span>, action <span class="arithmatex">\(a_t\)</span>, reward <span class="arithmatex">\(r_t\)</span>; expected return <span class="arithmatex">\(R_t = \sum \gamma^k r_{t+k}\)</span>; PQC for policy/value approximation; hybrid quantum-classical loop.</td>
</tr>
<tr>
<td><strong>13.2</strong></td>
<td><strong>Quantum Policy Gradient Methods</strong></td>
<td>PQC represents policy <span class="arithmatex">\(\pi_{\vec{\theta}}(a\|s)\)</span>; action sampling via measurement; policy gradient theorem: <span class="arithmatex">\(\nabla_{\vec{\theta}} J = \mathbb{E}[\nabla \log \pi \cdot R]\)</span>; Parameter Shift Rule for gradients.</td>
</tr>
<tr>
<td><strong>13.3</strong></td>
<td><strong>Quantum Value Iteration</strong></td>
<td>PQC approximates <span class="arithmatex">\(Q_{\vec{\theta}}(s,a)\)</span>; Quantum Bellman equation; TD error loss: <span class="arithmatex">\((Q - [r + \gamma Q'])^2\)</span>; VQ-DQN architecture with experience replay.</td>
</tr>
<tr>
<td><strong>13.4</strong></td>
<td><strong>Quantum Exploration Strategies</strong></td>
<td>Coherent exploration vs. <span class="arithmatex">\(\epsilon\)</span>-greedy; Quantum Random Walks for path diversity; amplitude amplification for optimal action reinforcement; entanglement in multi-agent coordination.</td>
</tr>
<tr>
<td><strong>13.5</strong></td>
<td><strong>Quantum Agent Architectures</strong></td>
<td>Hybrid components: PQC policy, measurement-based action, classical optimizer; quantum environments: simulators, adaptive experiments; multi-agent and generative architectures.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="131-qrl-basics"><strong>13.1 QRL Basics</strong></h2>
<hr />
<p>Quantum Reinforcement Learning (QRL) is a hybrid computational paradigm that integrates the decision-making framework of classical <strong>Reinforcement Learning (RL)</strong> with the computational power and unique resources of <strong>quantum mechanics</strong>. It seeks to develop agents that learn optimal behavior in an environment by maximizing a cumulative reward signal, using quantum circuits to model the underlying policies and value functions.</p>
<div class="admonition tip">
<p class="admonition-title">QRL's Hybrid Advantage</p>
<p>QRL doesn't replace classical RL entirelyit augments the function approximation step with quantum circuits. The exponential dimensionality of Hilbert space (<span class="arithmatex">\(2^n\)</span> for <span class="arithmatex">\(n\)</span> qubits) provides a fundamentally richer hypothesis space than classical neural networks with polynomial parameters [3].</p>
</div>
<h3 id="the-classical-rl-framework"><strong>The Classical RL Framework</strong></h3>
<hr />
<p>Classical RL is modeled as an interaction process over discrete timesteps, usually formulated as a Markov Decision Process (MDP).</p>
<p><strong>Interaction Loop</strong></p>
<p>At each time step <span class="arithmatex">\(t\)</span>, the <strong>agent</strong> observes the current state <span class="arithmatex">\(s_t\)</span>, selects an <strong>action</strong> <span class="arithmatex">\(a_t\)</span>, receives an immediate <strong>reward</strong> <span class="arithmatex">\(r_t\)</span>, and transitions to a new state <span class="arithmatex">\(s_{t+1}\)</span>.</p>
<p><strong>Goal</strong></p>
<p>The agent's objective is to maximize the <strong>expected return</strong> <span class="arithmatex">\(R_t\)</span>, which is the discounted sum of future rewards:</p>
<div class="arithmatex">\[
R_t = \sum_{k=0}^\infty \gamma^k r_{t+k}
\]</div>
<p>where <span class="arithmatex">\(\gamma \in [0, 1)\)</span> is the discount factor.</p>
<p><strong>Approximation Functions</strong></p>
<p>The agent learns by approximating either the <strong>policy</strong> (<span class="arithmatex">\(\pi(a|s)\)</span>) or the <strong>action-value function</strong> (<span class="arithmatex">\(Q(s, a)\)</span>), which guide decision-making.</p>
<pre class="mermaid"><code>flowchart LR
    A[State s] --&gt; B[Agent: Policy  or Q-function]
    B --&gt; C[Action a]
    C --&gt; D[Environment]
    D --&gt; E[Reward r]
    D --&gt; F[Next State s]
    E --&gt; G[Update Policy/Value]
    F --&gt; B</code></pre>
<hr />
<h3 id="the-qrl-extension-and-policy-representation"><strong>The QRL Extension and Policy Representation</strong></h3>
<hr />
<p>QRL extends the classical framework by utilizing quantum resources to represent the functions governing the agent's behavior.</p>
<p><strong>Quantum Representation</strong></p>
<p>States (<span class="arithmatex">\(s\)</span>) and actions (<span class="arithmatex">\(a\)</span>) are represented either as <strong>quantum states</strong> (using encoding techniques from Chapter 9) or are encoded into <strong>Parameterized Quantum Circuits (PQCs)</strong>.</p>
<p><strong>Policy/Value Approximation</strong></p>
<p>The crucial functionthe policy <span class="arithmatex">\(\pi(a|s)\)</span> or the value function <span class="arithmatex">\(Q(s, a)\)</span>is approximated by a PQC, often denoted as:</p>
<div class="arithmatex">\[
U(\vec{\theta})
\]</div>
<p>This PQC acts as a quantum function approximator, leveraging the high-dimensional Hilbert space and entanglement for potentially <strong>richer function approximation</strong>. The parameters <span class="arithmatex">\(\vec{\theta}\)</span> of the PQC are optimized iteratively via a classical feedback loop to maximize the expected return:</p>
<div class="arithmatex">\[
R_t
\]</div>
<div class="admonition example">
<p class="admonition-title">QRL Policy Circuit</p>
<p>For a grid-world environment with <span class="arithmatex">\(4 \times 4 = 16\)</span> states and 4 actions (up, down, left, right):</p>
<ol>
<li><strong>Encode state:</strong> Map state index to <span class="arithmatex">\(\log_2(16) = 4\)</span> qubits using basis encoding</li>
<li><strong>Apply PQC:</strong> Variational layers with parameters <span class="arithmatex">\(\vec{\theta}\)</span> create superposition over actions</li>
<li><strong>Measure:</strong> Measurement probabilities <span class="arithmatex">\(P(a) = |\langle a|\psi\rangle|^2\)</span> define policy <span class="arithmatex">\(\pi_{\vec{\theta}}(a|s)\)</span></li>
<li><strong>Sample action:</strong> Classical sampling from measurement distribution yields <span class="arithmatex">\(a_t\)</span></li>
</ol>
</div>
<hr />
<h3 id="the-hybrid-nature"><strong>The Hybrid Nature</strong></h3>
<hr />
<p>QRL models are fundamentally <strong>hybrid systems</strong>. The quantum computer handles the core computational tasks (encoding state information, generating action probabilities via measurement), while the classical computer manages the environment simulation and the optimization of the PQC parameters:</p>
<div class="arithmatex">\[
\vec{\theta}
\]</div>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>QRL_Training_Loop(environment, initial_theta, max_episodes):
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    theta = initial_theta
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    for episode in range(max_episodes):
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>        # Reset environment
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>        state = environment.reset()
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        episode_return = 0
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>        trajectory = []
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        while not episode.done:
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>            # Step 1: Encode state into quantum circuit
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>            quantum_state = Encode_State(state)
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>            # Step 2: Execute PQC with current parameters
</span><span id="__span-0-15"><a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>            pqc_output = Execute_PQC(quantum_state, theta)
</span><span id="__span-0-16"><a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
</span><span id="__span-0-17"><a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>            # Step 3: Measure to get action probabilities
</span><span id="__span-0-18"><a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>            action_probs = Measure_Circuit(pqc_output)
</span><span id="__span-0-19"><a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>
</span><span id="__span-0-20"><a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>            # Step 4: Sample action from distribution
</span><span id="__span-0-21"><a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>            action = Sample_Action(action_probs)
</span><span id="__span-0-22"><a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>
</span><span id="__span-0-23"><a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>            # Step 5: Execute action in environment
</span><span id="__span-0-24"><a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>            next_state, reward, done = environment.step(action)
</span><span id="__span-0-25"><a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>
</span><span id="__span-0-26"><a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>            # Step 6: Store transition
</span><span id="__span-0-27"><a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>            trajectory.append((state, action, reward))
</span><span id="__span-0-28"><a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>            episode_return += reward
</span><span id="__span-0-29"><a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>            state = next_state
</span><span id="__span-0-30"><a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>
</span><span id="__span-0-31"><a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        # Step 7: Compute gradient (policy gradient or TD error)
</span><span id="__span-0-32"><a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>        gradient = Compute_Gradient(trajectory, theta)
</span><span id="__span-0-33"><a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>
</span><span id="__span-0-34"><a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>        # Step 8: Classical optimizer updates parameters
</span><span id="__span-0-35"><a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>        theta = Classical_Optimizer_Update(theta, gradient)
</span><span id="__span-0-36"><a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>
</span><span id="__span-0-37"><a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>        # Step 9: Log performance
</span><span id="__span-0-38"><a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>        Log_Episode_Return(episode, episode_return)
</span><span id="__span-0-39"><a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>
</span><span id="__span-0-40"><a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>    return theta
</span></code></pre></div>
<hr />
<h2 id="132-quantum-policy-gradient-methods"><strong>13.2 Quantum Policy Gradient Methods</strong></h2>
<hr />
<p><strong>Quantum Policy Gradient (QPG) Methods</strong> are a class of Quantum Reinforcement Learning (QRL) algorithms that adapt the structure of classical policy optimization by using a <strong>Parameterized Quantum Circuit (PQC)</strong> to model the agent's stochastic policy. The goal is to maximize the expected cumulative reward by iteratively updating the PQC parameters based on the observed returns.</p>
<div class="admonition tip">
<p class="admonition-title">Why Policy Gradients in QRL?</p>
<p>Policy gradient methods are naturally suited to quantum implementation because they only require sampling from the policy (measurement) and computing gradients (Parameter Shift Rule). Unlike value-based methods, they don't require storing a Q-table, making them ideal for continuous or large action spaces [4].</p>
</div>
<h3 id="policy-representation-and-action-sampling"><strong>Policy Representation and Action Sampling</strong></h3>
<hr />
<p><strong>1. Quantum Policy Model</strong></p>
<p>The agent's stochastic policy, <span class="arithmatex">\(\pi_{\vec{\theta}}(a|s)\)</span> (the probability of taking action <span class="arithmatex">\(a\)</span> given state <span class="arithmatex">\(s\)</span>), is represented by a PQC:</p>
<div class="arithmatex">\[
U(\vec{\theta})
\]</div>
<p>The classical state <span class="arithmatex">\(s\)</span> is first encoded into the circuit.</p>
<p><strong>2. Action Probability</strong></p>
<p>The PQC is executed, and the probabilities of the possible actions are determined by measuring the output state in the computational basis. If the circuit outputs a state <span class="arithmatex">\(|\psi_s\rangle\)</span>, the probability of action <span class="arithmatex">\(a\)</span> is:</p>
<div class="arithmatex">\[
P(a) = |\langle a | \psi_s \rangle|^2
\]</div>
<p>This measurement-based sampling determines the agent's actual action.</p>
<p><strong>3. Superposition for Exploration</strong></p>
<p>QPG methods inherently leverage <strong>superposition</strong> to represent the probabilities of all possible actions simultaneously. This provides a basis for potentially enhanced <strong>state exploration</strong> and richer function approximation compared to purely classical methods.</p>
<hr />
<h3 id="the-quantum-policy-gradient-update"><strong>The Quantum Policy Gradient Update</strong></h3>
<hr />
<p>The PQC parameters <span class="arithmatex">\(\vec{\theta}\)</span> are optimized using the <strong>classical policy gradient theorem</strong>, adapted to calculate gradients from quantum measurements. The objective function <span class="arithmatex">\(J(\vec{\theta})\)</span> (the expected cumulative reward) is maximized by following the gradient:</p>
<div class="arithmatex">\[
\vec{\theta} \leftarrow \vec{\theta} + \alpha \nabla_{\vec{\theta}} J(\vec{\theta})
\]</div>
<p><strong>Policy Gradient Theorem</strong></p>
<p>The gradient <span class="arithmatex">\(\nabla_{\vec{\theta}} J(\vec{\theta})\)</span> is estimated using the formula (often referred to as the REINFORCE algorithm base):</p>
<div class="arithmatex">\[
\nabla_{\vec{\theta}} J(\vec{\theta}) = \mathbb{E}_{\pi} [\nabla_{\vec{\theta}} \log \pi_{\vec{\theta}}(a|s) \cdot R]
\]</div>
<p><strong>Interpretation</strong></p>
<p>The term <span class="arithmatex">\(\nabla_{\vec{\theta}} \log \pi_{\vec{\theta}}(a|s)\)</span> (<strong>score function</strong>) indicates the direction to adjust parameters to make the chosen action <span class="arithmatex">\(a\)</span> more likely. This direction is weighted by the <strong>return</strong> <span class="arithmatex">\(R\)</span> observed after the action. If <span class="arithmatex">\(R\)</span> is high, the action is reinforced (made more probable); if <span class="arithmatex">\(R\)</span> is low, it is discouraged.</p>
<hr />
<h3 id="gradient-calculation-and-efficiency"><strong>Gradient Calculation and Efficiency</strong></h3>
<hr />
<p><strong>1. Quantum Gradient Estimation</strong></p>
<p>The gradient components, <span class="arithmatex">\(\nabla_{\vec{\theta}} \log \pi_{\vec{\theta}}(a|s)\)</span>, must be calculated efficiently on the quantum hardware. This is achieved using the <strong>Parameter Shift Rule</strong> (introduced in Chapter 10), which allows the calculation of the gradient of the measured expectation value by running the PQC multiple times with shifted parameters.</p>
<p><strong>2. Hybrid Optimization</strong></p>
<p>The calculated gradient is then fed to a <strong>classical optimizer</strong> (e.g., Adam or gradient descent) to perform the parameter update. This hybrid approach manages the high measurement noise inherent in quantum gradient estimation.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>Quantum_Policy_Gradient(environment, initial_theta, num_episodes):
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>    theta = initial_theta
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    for episode in range(num_episodes):
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>        # Collect trajectory using current policy
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>        trajectory = []
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>        state = environment.reset()
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>        while not done:
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>            # Encode state and execute PQC
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>            quantum_state = Encode_State(state)
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>            pqc_output = PQC(quantum_state, theta)
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>            # Measure to get action probabilities
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>            action_probs = Measure(pqc_output)
</span><span id="__span-1-16"><a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>            action = Sample(action_probs)
</span><span id="__span-1-17"><a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>
</span><span id="__span-1-18"><a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>            # Environment step
</span><span id="__span-1-19"><a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>            next_state, reward, done = environment.step(action)
</span><span id="__span-1-20"><a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>            trajectory.append((state, action, reward))
</span><span id="__span-1-21"><a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>            state = next_state
</span><span id="__span-1-22"><a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>
</span><span id="__span-1-23"><a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>        # Compute returns for each timestep
</span><span id="__span-1-24"><a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>        returns = Compute_Discounted_Returns(trajectory)
</span><span id="__span-1-25"><a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>
</span><span id="__span-1-26"><a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>        # Compute policy gradient using Parameter Shift Rule
</span><span id="__span-1-27"><a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>        gradient = zeros(len(theta))
</span><span id="__span-1-28"><a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>
</span><span id="__span-1-29"><a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>        for t, (state, action, reward) in enumerate(trajectory):
</span><span id="__span-1-30"><a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>            # Score function gradient
</span><span id="__span-1-31"><a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a>            for k in range(len(theta)):
</span><span id="__span-1-32"><a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a>                # Parameter Shift Rule
</span><span id="__span-1-33"><a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a>                theta_plus = theta.copy()
</span><span id="__span-1-34"><a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a>                theta_plus[k] += pi/2
</span><span id="__span-1-35"><a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a>                theta_minus = theta.copy()
</span><span id="__span-1-36"><a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a>                theta_minus[k] -= pi/2
</span><span id="__span-1-37"><a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a>
</span><span id="__span-1-38"><a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a>                # Compute log probability gradients
</span><span id="__span-1-39"><a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a>                log_pi_plus = Log_Policy_Prob(state, action, theta_plus)
</span><span id="__span-1-40"><a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a>                log_pi_minus = Log_Policy_Prob(state, action, theta_minus)
</span><span id="__span-1-41"><a id="__codelineno-1-41" name="__codelineno-1-41" href="#__codelineno-1-41"></a>
</span><span id="__span-1-42"><a id="__codelineno-1-42" name="__codelineno-1-42" href="#__codelineno-1-42"></a>                # Gradient approximation
</span><span id="__span-1-43"><a id="__codelineno-1-43" name="__codelineno-1-43" href="#__codelineno-1-43"></a>                grad_log_pi = (log_pi_plus - log_pi_minus) / 2
</span><span id="__span-1-44"><a id="__codelineno-1-44" name="__codelineno-1-44" href="#__codelineno-1-44"></a>
</span><span id="__span-1-45"><a id="__codelineno-1-45" name="__codelineno-1-45" href="#__codelineno-1-45"></a>                # Weight by return
</span><span id="__span-1-46"><a id="__codelineno-1-46" name="__codelineno-1-46" href="#__codelineno-1-46"></a>                gradient[k] += grad_log_pi * returns[t]
</span><span id="__span-1-47"><a id="__codelineno-1-47" name="__codelineno-1-47" href="#__codelineno-1-47"></a>
</span><span id="__span-1-48"><a id="__codelineno-1-48" name="__codelineno-1-48" href="#__codelineno-1-48"></a>        # Average over trajectory
</span><span id="__span-1-49"><a id="__codelineno-1-49" name="__codelineno-1-49" href="#__codelineno-1-49"></a>        gradient /= len(trajectory)
</span><span id="__span-1-50"><a id="__codelineno-1-50" name="__codelineno-1-50" href="#__codelineno-1-50"></a>
</span><span id="__span-1-51"><a id="__codelineno-1-51" name="__codelineno-1-51" href="#__codelineno-1-51"></a>        # Update parameters
</span><span id="__span-1-52"><a id="__codelineno-1-52" name="__codelineno-1-52" href="#__codelineno-1-52"></a>        learning_rate = 0.01
</span><span id="__span-1-53"><a id="__codelineno-1-53" name="__codelineno-1-53" href="#__codelineno-1-53"></a>        theta = theta + learning_rate * gradient
</span><span id="__span-1-54"><a id="__codelineno-1-54" name="__codelineno-1-54" href="#__codelineno-1-54"></a>
</span><span id="__span-1-55"><a id="__codelineno-1-55" name="__codelineno-1-55" href="#__codelineno-1-55"></a>    return theta
</span></code></pre></div>
<details class="question">
<summary>Why use the Parameter Shift Rule instead of classical finite differences?</summary>
<p>The Parameter Shift Rule exploits the analytic structure of quantum gates to compute exact gradients (up to sampling noise) without approximation error. Classical finite differences require choosing a step size <span class="arithmatex">\(\epsilon\)</span> and suffer from truncation error. For quantum circuits, the Parameter Shift Rule is both more accurate and naturally suited to the hardware.</p>
</details>
<hr />
<h2 id="133-quantum-value-iteration"><strong>13.3 Quantum Value Iteration</strong></h2>
<hr />
<p><strong>Quantum Value Iteration</strong> (QVI) encompasses QRL methods where the primary goal is to approximate the <strong>optimal value function</strong>, <span class="arithmatex">\(V^*(s)\)</span>, or the <strong>optimal action-value function</strong>, <span class="arithmatex">\(Q^*(s, a)\)</span>, using parameterized quantum circuits (PQCs). These methods are foundational to Q-Learning and Deep Q-Networks (DQNs) in the quantum domain.</p>
<div class="admonition tip">
<p class="admonition-title">Value-Based vs. Policy-Based QRL</p>
<p>Value-based methods (QVI) learn to estimate the value of states/actions, then derive policy greedily. Policy-based methods (QPG) directly optimize the policy. QVI is better when you need explicit value estimates; QPG is better for continuous actions or stochastic optimal policies [5].</p>
</div>
<h3 id="value-function-approximation"><strong>Value Function Approximation</strong></h3>
<hr />
<p>In QVI, the value functions are approximated by a PQC, <span class="arithmatex">\(Q_{\vec{\theta}}(s, a)\)</span>, where <span class="arithmatex">\(\vec{\theta}\)</span> are the trainable parameters.</p>
<p><strong>Goal</strong></p>
<p>The PQC is trained to minimize the difference between its current estimate of the Q-value and a more stable estimate of the optimal future value, a concept formalized by the <strong>Quantum Bellman Equation</strong>.</p>
<p><strong>Methodology</strong></p>
<p>These approaches are often referred to as <strong>Quantum Q-Learning</strong> or <strong>Variational Quantum Deep Q-Networks (VQ-DQN)</strong>, replacing the core neural network approximator of a classical DQN with a PQC.</p>
<hr />
<h3 id="the-quantum-bellman-equation-and-temporal-difference-error"><strong>The Quantum Bellman Equation and Temporal-Difference Error</strong></h3>
<hr />
<p>The recursive relationship defining the optimal Q-value, <span class="arithmatex">\(Q^*(s, a)\)</span>, forms the basis of the QVI loss function:</p>
<p><strong>1. Classical Bellman Optimality Equation (for Q-values)</strong></p>
<p>The optimal value of taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> is the immediate reward <span class="arithmatex">\(r\)</span> plus the discounted maximum expected Q-value of the next state <span class="arithmatex">\(s'\)</span>:</p>
<div class="arithmatex">\[
Q^*(s, a) = r + \gamma \max_{a'} Q^*(s', a')
\]</div>
<p><strong>2. Temporal-Difference (TD) Error Loss</strong></p>
<p>In QVI, the PQC is trained to minimize the squared difference between its prediction, <span class="arithmatex">\(Q_{\vec{\theta}}(s, a)\)</span>, and the <strong>TD target</strong> (the stable estimate of the right side of the Bellman equation):</p>
<div class="arithmatex">\[
\mathcal{L}(\vec{\theta}) = \left(Q_{\vec{\theta}}(s, a) - \left[r + \gamma \cdot Q_{\vec{\theta}}(s', a')\right]\right)^2
\]</div>
<p>The term:</p>
<div class="arithmatex">\[
r + \gamma \cdot Q_{\vec{\theta}}(s', a')
\]</div>
<p>represents the estimated optimal Q-value of the next state, and its difference from the current estimate, <span class="arithmatex">\(Q_{\vec{\theta}}(s, a)\)</span>, is the <strong>temporal-difference error</strong>.</p>
<p><strong>3. Hybrid Optimization</strong></p>
<p>The loss <span class="arithmatex">\(\mathcal{L}\)</span> is minimized using a classical optimizer (e.g., gradient descent or Adam), which updates the PQC parameters <span class="arithmatex">\(\vec{\theta}\)</span> in a hybrid variational loop.</p>
<hr />
<h3 id="quantum-deep-q-networks-vq-dqn"><strong>Quantum Deep Q-Networks (VQ-DQN)</strong></h3>
<hr />
<p>The VQ-DQN architecture is a direct quantum adaptation of the Deep Q-Network:</p>
<p><strong>VQC Component</strong></p>
<p>The neural network traditionally used to approximate the Q-function is replaced by a <strong>Variational Quantum Circuit (VQC)</strong>. This VQC encodes the state <span class="arithmatex">\(s\)</span> and outputs the estimated Q-values.</p>
<p><strong>Stabilization Techniques</strong></p>
<p>VQ-DQN incorporates classical stabilization techniques necessary for deep learning, such as <strong>experience replay</strong> (storing past interactions in a buffer) and <strong>target networks</strong> (a separate, slowly updated network used to calculate the stable TD target). These techniques are crucial for mitigating instability and ensuring convergence.</p>
<p><strong>Action Selection</strong></p>
<p>The agent uses the approximated <span class="arithmatex">\(Q_{\vec{\theta}}(s, a)\)</span> values to select the optimal action <span class="arithmatex">\(a\)</span> by applying the <span class="arithmatex">\(\epsilon\)</span>-greedy strategy, balancing <strong>exploration</strong> and <strong>exploitation</strong>.</p>
<p>The goal of VQ-DQN is to leverage quantum resources, such as superposition, to potentially process high-dimensional state spaces more efficiently than their classical counterparts.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>VQ_DQN_Training(environment, initial_theta, max_steps):
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>    theta = initial_theta  # Main network parameters
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>    theta_target = theta.copy()  # Target network parameters
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    replay_buffer = []
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    state = environment.reset()
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    for step in range(max_steps):
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>        # Step 1: Epsilon-greedy action selection
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>        if random() &lt; epsilon:
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>            action = Random_Action()
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>        else:
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>            # Compute Q-values using VQC
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>            q_values = []
</span><span id="__span-2-15"><a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>            for a in range(num_actions):
</span><span id="__span-2-16"><a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>                q_state_action = VQC_Q_Value(state, a, theta)
</span><span id="__span-2-17"><a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>                q_values.append(q_state_action)
</span><span id="__span-2-18"><a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>            action = argmax(q_values)
</span><span id="__span-2-19"><a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>
</span><span id="__span-2-20"><a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>        # Step 2: Execute action in environment
</span><span id="__span-2-21"><a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>        next_state, reward, done = environment.step(action)
</span><span id="__span-2-22"><a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a>
</span><span id="__span-2-23"><a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a>        # Step 3: Store transition in replay buffer
</span><span id="__span-2-24"><a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a>        replay_buffer.append((state, action, reward, next_state, done))
</span><span id="__span-2-25"><a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a>        if len(replay_buffer) &gt; buffer_size:
</span><span id="__span-2-26"><a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a>            replay_buffer.pop(0)
</span><span id="__span-2-27"><a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a>
</span><span id="__span-2-28"><a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a>        # Step 4: Sample mini-batch from replay buffer
</span><span id="__span-2-29"><a id="__codelineno-2-29" name="__codelineno-2-29" href="#__codelineno-2-29"></a>        if len(replay_buffer) &gt;= batch_size:
</span><span id="__span-2-30"><a id="__codelineno-2-30" name="__codelineno-2-30" href="#__codelineno-2-30"></a>            batch = Sample_Batch(replay_buffer, batch_size)
</span><span id="__span-2-31"><a id="__codelineno-2-31" name="__codelineno-2-31" href="#__codelineno-2-31"></a>
</span><span id="__span-2-32"><a id="__codelineno-2-32" name="__codelineno-2-32" href="#__codelineno-2-32"></a>            # Step 5: Compute TD error for batch
</span><span id="__span-2-33"><a id="__codelineno-2-33" name="__codelineno-2-33" href="#__codelineno-2-33"></a>            total_loss = 0
</span><span id="__span-2-34"><a id="__codelineno-2-34" name="__codelineno-2-34" href="#__codelineno-2-34"></a>            for (s, a, r, s_next, d) in batch:
</span><span id="__span-2-35"><a id="__codelineno-2-35" name="__codelineno-2-35" href="#__codelineno-2-35"></a>                # Current Q-value
</span><span id="__span-2-36"><a id="__codelineno-2-36" name="__codelineno-2-36" href="#__codelineno-2-36"></a>                q_current = VQC_Q_Value(s, a, theta)
</span><span id="__span-2-37"><a id="__codelineno-2-37" name="__codelineno-2-37" href="#__codelineno-2-37"></a>
</span><span id="__span-2-38"><a id="__codelineno-2-38" name="__codelineno-2-38" href="#__codelineno-2-38"></a>                # Target Q-value (using target network)
</span><span id="__span-2-39"><a id="__codelineno-2-39" name="__codelineno-2-39" href="#__codelineno-2-39"></a>                if d:  # Terminal state
</span><span id="__span-2-40"><a id="__codelineno-2-40" name="__codelineno-2-40" href="#__codelineno-2-40"></a>                    q_target = r
</span><span id="__span-2-41"><a id="__codelineno-2-41" name="__codelineno-2-41" href="#__codelineno-2-41"></a>                else:
</span><span id="__span-2-42"><a id="__codelineno-2-42" name="__codelineno-2-42" href="#__codelineno-2-42"></a>                    q_next_values = []
</span><span id="__span-2-43"><a id="__codelineno-2-43" name="__codelineno-2-43" href="#__codelineno-2-43"></a>                    for a_next in range(num_actions):
</span><span id="__span-2-44"><a id="__codelineno-2-44" name="__codelineno-2-44" href="#__codelineno-2-44"></a>                        q_val = VQC_Q_Value(s_next, a_next, theta_target)
</span><span id="__span-2-45"><a id="__codelineno-2-45" name="__codelineno-2-45" href="#__codelineno-2-45"></a>                        q_next_values.append(q_val)
</span><span id="__span-2-46"><a id="__codelineno-2-46" name="__codelineno-2-46" href="#__codelineno-2-46"></a>                    q_target = r + gamma * max(q_next_values)
</span><span id="__span-2-47"><a id="__codelineno-2-47" name="__codelineno-2-47" href="#__codelineno-2-47"></a>
</span><span id="__span-2-48"><a id="__codelineno-2-48" name="__codelineno-2-48" href="#__codelineno-2-48"></a>                # TD error loss
</span><span id="__span-2-49"><a id="__codelineno-2-49" name="__codelineno-2-49" href="#__codelineno-2-49"></a>                loss = (q_current - q_target)**2
</span><span id="__span-2-50"><a id="__codelineno-2-50" name="__codelineno-2-50" href="#__codelineno-2-50"></a>                total_loss += loss
</span><span id="__span-2-51"><a id="__codelineno-2-51" name="__codelineno-2-51" href="#__codelineno-2-51"></a>
</span><span id="__span-2-52"><a id="__codelineno-2-52" name="__codelineno-2-52" href="#__codelineno-2-52"></a>            # Step 6: Compute gradient and update main network
</span><span id="__span-2-53"><a id="__codelineno-2-53" name="__codelineno-2-53" href="#__codelineno-2-53"></a>            gradient = Compute_Gradient_Loss(total_loss, theta)
</span><span id="__span-2-54"><a id="__codelineno-2-54" name="__codelineno-2-54" href="#__codelineno-2-54"></a>            theta = theta - learning_rate * gradient
</span><span id="__span-2-55"><a id="__codelineno-2-55" name="__codelineno-2-55" href="#__codelineno-2-55"></a>
</span><span id="__span-2-56"><a id="__codelineno-2-56" name="__codelineno-2-56" href="#__codelineno-2-56"></a>        # Step 7: Periodically update target network
</span><span id="__span-2-57"><a id="__codelineno-2-57" name="__codelineno-2-57" href="#__codelineno-2-57"></a>        if step % target_update_freq == 0:
</span><span id="__span-2-58"><a id="__codelineno-2-58" name="__codelineno-2-58" href="#__codelineno-2-58"></a>            theta_target = theta.copy()
</span><span id="__span-2-59"><a id="__codelineno-2-59" name="__codelineno-2-59" href="#__codelineno-2-59"></a>
</span><span id="__span-2-60"><a id="__codelineno-2-60" name="__codelineno-2-60" href="#__codelineno-2-60"></a>        # Step 8: Reset if episode done
</span><span id="__span-2-61"><a id="__codelineno-2-61" name="__codelineno-2-61" href="#__codelineno-2-61"></a>        if done:
</span><span id="__span-2-62"><a id="__codelineno-2-62" name="__codelineno-2-62" href="#__codelineno-2-62"></a>            state = environment.reset()
</span><span id="__span-2-63"><a id="__codelineno-2-63" name="__codelineno-2-63" href="#__codelineno-2-63"></a>        else:
</span><span id="__span-2-64"><a id="__codelineno-2-64" name="__codelineno-2-64" href="#__codelineno-2-64"></a>            state = next_state
</span><span id="__span-2-65"><a id="__codelineno-2-65" name="__codelineno-2-65" href="#__codelineno-2-65"></a>
</span><span id="__span-2-66"><a id="__codelineno-2-66" name="__codelineno-2-66" href="#__codelineno-2-66"></a>    return theta
</span></code></pre></div>
<div class="admonition example">
<p class="admonition-title">VQ-DQN for CartPole</p>
<p>Classic CartPole environment: 4D continuous state (position, velocity, angle, angular velocity), 2 discrete actions (left, right).</p>
<ul>
<li><strong>State encoding:</strong> Amplitude encoding of 4 features into 2 qubits</li>
<li><strong>VQC:</strong> 3-layer ansatz with <span class="arithmatex">\(RY\)</span>, <span class="arithmatex">\(RZ\)</span> rotations and CNOT entanglement</li>
<li><strong>Q-value extraction:</strong> Two separate measurements or two output qubits for <span class="arithmatex">\(Q(s, \text{left})\)</span> and <span class="arithmatex">\(Q(s, \text{right})\)</span></li>
<li><strong>Training:</strong> Experience replay buffer with 10,000 transitions, target network updated every 100 steps</li>
</ul>
</div>
<hr />
<h2 id="134-quantum-exploration-strategies"><strong>13.4 Quantum Exploration Strategies</strong></h2>
<hr />
<p>Exploration is fundamental to reinforcement learningwithout exploring the state-action space, an agent cannot discover optimal policies. Classical RL uses stochastic exploration strategies like <span class="arithmatex">\(\epsilon\)</span>-greedy or Boltzmann exploration. Quantum RL introduces <strong>coherent exploration</strong> mechanisms that exploit quantum superposition and interference to navigate state spaces in fundamentally different ways.</p>
<div class="admonition tip">
<p class="admonition-title">Coherent vs. Stochastic Exploration</p>
<p>Classical exploration adds random noise to decisions. Quantum exploration maintains coherent superposition over exploration paths, allowing interference to bias the search toward promising regions. This is the difference between flipping coins versus quantum walking [6].</p>
</div>
<h3 id="non-classical-exploration-mechanisms"><strong>Non-Classical Exploration Mechanisms</strong></h3>
<hr />
<p>Classical exploration strategies rely on introducing random (probabilistic) actions to escape local optima. QRL replaces this purely random approach with <strong>coherent exploration</strong>, where the search paths are maintained in a quantum superposition, allowing for directed sampling.</p>
<p><strong>Quantum Random Walks (QRW)</strong></p>
<p>The quantum analogue of a classical random walk.</p>
<p><em>Function:</em> QRWs are used to achieve <strong>more diverse path exploration</strong>. Unlike a classical walk, which follows one path, a QRW explores multiple paths simultaneously in superposition, biased by interference.</p>
<p><em>Advantage:</em> This method can lead to faster <strong>hitting times</strong> (reaching a specific high-reward state) compared to the purely probabilistic nature of classical walks.</p>
<p><strong>Amplitude Amplification</strong></p>
<p>This technique (generalized from Grover's algorithm, Chapter 4) can be incorporated into the policy to favor high-reward actions.</p>
<p><em>Function:</em> If the agent has observed a state-action pair that led to a high reward, amplitude amplification can be used to <strong>increase the probability amplitude</strong> of selecting that optimal action in similar future states.</p>
<p><em>Advantage:</em> This allows the agent to search for and reinforce optimal actions in a targeted, quantum-enhanced manner, potentially improving the convergence rate of the policy.</p>
<p><strong>Entanglement for Multi-Agent Systems</strong></p>
<p>Entanglement can be used in multi-agent QRL systems to establish non-classical correlations between agents' exploration policies, potentially allowing for <strong>coordinated and decentralized exploration</strong> strategies that are superior to classical coordination.</p>
<pre class="mermaid"><code>flowchart TD
    A[Exploration Strategy] --&gt; B{Classical or Quantum?}
    B --&gt;|Classical| C["-greedy: Random Action"]
    B --&gt;|Quantum| D[Coherent Exploration]
    C --&gt; E[Uncorrelated Random Sampling]
    D --&gt; F[Quantum Random Walk]
    D --&gt; G[Amplitude Amplification]
    D --&gt; H[Entangled Multi-Agent]
    F --&gt; I[Faster Hitting Times]
    G --&gt; J[Targeted Optimal Action Search]
    H --&gt; K[Coordinated Decentralized Exploration]</code></pre>
<hr />
<h3 id="coherent-exploration-vs-epsilon-greedy"><strong>Coherent Exploration vs. <span class="arithmatex">\(\epsilon\)</span>-Greedy</strong></h3>
<hr />
<p>The fundamental difference lies in the nature of the uncertainty introduced:</p>
<p><strong>Classical (<span class="arithmatex">\(\epsilon\)</span>-Greedy)</strong></p>
<p>When an agent chooses a random action (with probability <span class="arithmatex">\(\epsilon\)</span>), that choice is purely random and uncorrelated with previous exploration paths.</p>
<p><strong>Quantum (Coherent)</strong></p>
<p>Quantum random walks and amplitude amplification maintain <strong>coherence</strong> over the explored path space. The decision process is not random noise but a <strong>controlled superposition</strong> that is weighted by interference, allowing the agent to search the state space more efficiently and purposefully.</p>
<p>These non-classical strategies provide a foundation for developing QRL algorithms that can more efficiently navigate large, complex state spaces, enhancing the agent's ability to find the maximum cumulative reward.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>Quantum_Random_Walk_Exploration(graph, start_node, target_node, num_steps):
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>    # Initialize walker in superposition over all nodes
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>    num_nodes = len(graph.nodes)
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>    walker_state = Uniform_Superposition(num_nodes)
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>    # Coin operator (creates superposition over move directions)
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    coin_dim = max([graph.degree(node) for node in graph.nodes])
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>    for step in range(num_steps):
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>        # Step 1: Apply coin operator (Hadamard on coin space)
</span><span id="__span-3-11"><a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>        walker_state = Apply_Coin_Operator(walker_state, coin_dim)
</span><span id="__span-3-12"><a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>
</span><span id="__span-3-13"><a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>        # Step 2: Apply shift operator (move based on coin)
</span><span id="__span-3-14"><a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a>        walker_state = Apply_Shift_Operator(walker_state, graph)
</span><span id="__span-3-15"><a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>
</span><span id="__span-3-16"><a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a>        # Optional: Measure to check if target reached
</span><span id="__span-3-17"><a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a>        if step % check_interval == 0:
</span><span id="__span-3-18"><a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a>            measurement = Measure_Position(walker_state)
</span><span id="__span-3-19"><a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a>            if measurement == target_node:
</span><span id="__span-3-20"><a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a>                return step  # Hitting time
</span><span id="__span-3-21"><a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a>
</span><span id="__span-3-22"><a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a>    # Final measurement to extract node
</span><span id="__span-3-23"><a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a>    final_node = Measure_Position(walker_state)
</span><span id="__span-3-24"><a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a>    return final_node
</span><span id="__span-3-25"><a id="__codelineno-3-25" name="__codelineno-3-25" href="#__codelineno-3-25"></a>
</span><span id="__span-3-26"><a id="__codelineno-3-26" name="__codelineno-3-26" href="#__codelineno-3-26"></a>Amplitude_Amplification_Policy(state, high_reward_actions, num_iterations):
</span><span id="__span-3-27"><a id="__codelineno-3-27" name="__codelineno-3-27" href="#__codelineno-3-27"></a>    # Initialize uniform superposition over all actions
</span><span id="__span-3-28"><a id="__codelineno-3-28" name="__codelineno-3-28" href="#__codelineno-3-28"></a>    num_actions = len(all_actions)
</span><span id="__span-3-29"><a id="__codelineno-3-29" name="__codelineno-3-29" href="#__codelineno-3-29"></a>    action_state = Uniform_Superposition(num_actions)
</span><span id="__span-3-30"><a id="__codelineno-3-30" name="__codelineno-3-30" href="#__codelineno-3-30"></a>
</span><span id="__span-3-31"><a id="__codelineno-3-31" name="__codelineno-3-31" href="#__codelineno-3-31"></a>    for iteration in range(num_iterations):
</span><span id="__span-3-32"><a id="__codelineno-3-32" name="__codelineno-3-32" href="#__codelineno-3-32"></a>        # Step 1: Oracle marks high-reward actions
</span><span id="__span-3-33"><a id="__codelineno-3-33" name="__codelineno-3-33" href="#__codelineno-3-33"></a>        action_state = Apply_Oracle(action_state, high_reward_actions)
</span><span id="__span-3-34"><a id="__codelineno-3-34" name="__codelineno-3-34" href="#__codelineno-3-34"></a>
</span><span id="__span-3-35"><a id="__codelineno-3-35" name="__codelineno-3-35" href="#__codelineno-3-35"></a>        # Step 2: Diffusion operator amplifies marked amplitudes
</span><span id="__span-3-36"><a id="__codelineno-3-36" name="__codelineno-3-36" href="#__codelineno-3-36"></a>        action_state = Apply_Diffusion(action_state)
</span><span id="__span-3-37"><a id="__codelineno-3-37" name="__codelineno-3-37" href="#__codelineno-3-37"></a>
</span><span id="__span-3-38"><a id="__codelineno-3-38" name="__codelineno-3-38" href="#__codelineno-3-38"></a>    # Measure to get action with amplified probability
</span><span id="__span-3-39"><a id="__codelineno-3-39" name="__codelineno-3-39" href="#__codelineno-3-39"></a>    action = Measure_Action(action_state)
</span><span id="__span-3-40"><a id="__codelineno-3-40" name="__codelineno-3-40" href="#__codelineno-3-40"></a>    return action
</span></code></pre></div>
<details class="question">
<summary>How much speedup can Quantum Random Walks provide over classical random walks?</summary>
<p>For certain graph structures, QRWs can achieve quadratic speedup in hitting time. For example, on a hypercube, classical random walk requires <span class="arithmatex">\(O(N \log N)\)</span> steps to hit all vertices, while QRW requires <span class="arithmatex">\(O(\sqrt{N} \log N)\)</span> steps. However, the speedup is graph-dependent and not universal.</p>
</details>
<hr />
<h2 id="135-quantum-agent-architectures"><strong>13.5 Quantum Agent Architectures</strong></h2>
<hr />
<p>A <strong>Quantum Agent Architecture</strong> defines the structure of the computational components that govern a QRL agent's interaction with its environment. The architecture is inherently a <strong>hybrid system</strong>, utilizing classical processors for iterative learning and optimization, while reserving quantum resources for complex state representation and action generation.</p>
<div class="admonition tip">
<p class="admonition-title">Hybrid Architecture Necessity</p>
<p>Current quantum hardware cannot run full RL loops autonomously. The hybrid approach leverages quantum advantage where it matters most (function approximation in exponential space) while using classical processors for tasks they excel at (optimization, environment simulation, memory management) [7].</p>
</div>
<h3 id="core-hybrid-components"><strong>Core Hybrid Components</strong></h3>
<hr />
<p>The quantum agent is characterized by the implementation method for its key functional components:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component</th>
<th style="text-align: left;">Quantum Implementation</th>
<th style="text-align: left;">Role in Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Policy / Value Function</strong></td>
<td style="text-align: left;"><strong>Parameterized Quantum Circuit (PQC)</strong></td>
<td style="text-align: left;">Approximates the stochastic policy <span class="arithmatex">\(\pi(a\|s)\)</span> or the value function <span class="arithmatex">\(Q(s, a)\)</span>. The parameters (<span class="arithmatex">\(\vec{\theta}\)</span>) are optimized.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>State Encoding</strong></td>
<td style="text-align: left;">Quantum states (encoded via amplitude or angle techniques)</td>
<td style="text-align: left;">Translates the observed state <span class="arithmatex">\(s_t\)</span> into a high-dimensional quantum feature state for processing.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Action Generation</strong></td>
<td style="text-align: left;"><strong>Measurement-based sampling</strong></td>
<td style="text-align: left;">The final output state of the PQC is measured in the computational basis. The resulting bit string is sampled probabilistically, yielding the definitive action <span class="arithmatex">\(a_t\)</span>.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Memory</strong></td>
<td style="text-align: left;">Quantum states or <strong>QRAM</strong> (Quantum Random Access Memory)</td>
<td style="text-align: left;">Stores information such as past states, policies, or the approximated Q-table. QRAM is a proposed future technology for efficient quantum data storage.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Learning Mechanism</strong></td>
<td style="text-align: left;"><strong>Classical gradient-based updates</strong></td>
<td style="text-align: left;">The gradient of the objective function (e.g., policy gradient or TD error) is computed and used by a <strong>classical optimizer</strong> to update the PQC parameters <span class="arithmatex">\(\vec{\theta}\)</span>.</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="the-quantum-environment"><strong>The Quantum Environment</strong></h3>
<hr />
<p>The nature of the agent's <strong>environment</strong> is highly flexible in the QRL paradigm, ranging from classical simulations to real-world quantum physics experiments:</p>
<p><strong>Classical Environment</strong></p>
<p>The environment is simulated on a conventional computer (e.g., a simple grid world or a complex game). The agent's quantum state output is measured, converted to a classical action, and the environment returns a classical state <span class="arithmatex">\(s_{t+1}\)</span> and reward <span class="arithmatex">\(r_t\)</span>.</p>
<p><strong>Quantum Simulator</strong></p>
<p>The environment itself is modeled by a second quantum circuit or simulator, allowing the agent to learn the dynamics of a quantum mechanical system.</p>
<p><strong>Quantum Physical System (Adaptive Experiments)</strong></p>
<p>In advanced applications, the agent interacts directly with a <strong>physical quantum device</strong>. The environment's state (<span class="arithmatex">\(s_t\)</span>) might be a measurement of the device's fidelity, and the action (<span class="arithmatex">\(a_t\)</span>) might be a parameter adjustment (e.g., adjusting laser pulse sequences), allowing the agent to perform <strong>adaptive quantum experiments</strong> autonomously.</p>
<div class="admonition example">
<p class="admonition-title">Adaptive Quantum Control</p>
<p>Task: Optimize quantum gate fidelity by adjusting pulse parameters.</p>
<ul>
<li><strong>State <span class="arithmatex">\(s_t\)</span>:</strong> Current gate fidelity estimate from process tomography</li>
<li><strong>Action <span class="arithmatex">\(a_t\)</span>:</strong> Adjustment to pulse amplitude, duration, or phase</li>
<li><strong>Reward <span class="arithmatex">\(r_t\)</span>:</strong> Negative infidelity: <span class="arithmatex">\(r_t = -(1 - F_{\text{gate}})\)</span></li>
<li><strong>QRL agent:</strong> Learns optimal pulse sequences faster than grid search or gradient-free optimization</li>
<li><strong>Result:</strong> Automated calibration of quantum hardware</li>
</ul>
</div>
<hr />
<h3 id="advanced-architectures"><strong>Advanced Architectures</strong></h3>
<hr />
<p>Beyond the single-agent PQC, QRL research extends to more complex structures:</p>
<p><strong>Multi-agent QRL</strong></p>
<p>Systems where multiple agents, potentially coordinated by <strong>entanglement</strong> (as discussed in Section 13.4), learn and interact simultaneously within a shared or distributed quantum environment.</p>
<p><strong>Quantum Generative Models</strong></p>
<p>Architectures that incorporate Quantum Generative Adversarial Networks (QGANs) or Quantum Boltzmann Machines (QBMs) as components for advanced environment modeling or robust policy generation.</p>
<pre class="mermaid"><code>flowchart TB
    A[Quantum Agent Architecture] --&gt; B[Single Agent]
    A --&gt; C[Multi-Agent]
    B --&gt; D[PQC Policy]
    B --&gt; E[Classical Optimizer]
    B --&gt; F[Environment: Classical/Quantum/Physical]
    C --&gt; G[Entangled Coordination]
    C --&gt; H[Decentralized Learning]
    D --&gt; I[State Encoding]
    D --&gt; J[Action Sampling]
    E --&gt; K[Gradient Computation]
    E --&gt; L[Parameter Update]</code></pre>
<hr />
<h2 id="summary-quantum-reinforcement-learning-paradigms"><strong>Summary: Quantum Reinforcement Learning Paradigms</strong></h2>
<hr />
<h3 id="i-core-paradigms-and-objectives"><strong>I. Core Paradigms and Objectives</strong></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">QRL Paradigm</th>
<th style="text-align: left;">Classical Inspiration</th>
<th style="text-align: left;">Primary Quantum Objective</th>
<th style="text-align: left;">Loss Function / Optimization Target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Quantum Policy Gradient (QPG)</strong></td>
<td style="text-align: left;">Policy Gradient (REINFORCE)</td>
<td style="text-align: left;">Maximize expected return <span class="arithmatex">\(J(\vec{\theta})\)</span> by training PQC as optimal policy <span class="arithmatex">\(\pi_{\vec{\theta}}(a\|s)\)</span></td>
<td style="text-align: left;"><strong>Policy Gradient Theorem:</strong> <span class="arithmatex">\(\nabla_{\vec{\theta}} J = \mathbb{E}_{\pi}[\nabla_{\vec{\theta}} \log \pi_{\vec{\theta}}(a\|s) \cdot R]\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quantum Value Iteration (QVI)</strong></td>
<td style="text-align: left;">Q-Learning, DQN</td>
<td style="text-align: left;">Minimize difference between current Q-value and optimal future value <span class="arithmatex">\(Q^*(s, a)\)</span></td>
<td style="text-align: left;"><strong>TD Error Loss:</strong> <span class="arithmatex">\(\mathcal{L} = (Q_{\vec{\theta}}(s, a) - [r + \gamma \cdot Q_{\vec{\theta}}(s', a')])^2\)</span></td>
</tr>
</tbody>
</table>
<h3 id="ii-mechanism-and-resource-utilization"><strong>II. Mechanism and Resource Utilization</strong></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Component</th>
<th style="text-align: left;">Quantum Implementation</th>
<th style="text-align: left;">Role of Quantum Resource</th>
<th style="text-align: left;">Measurement Requirement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Function Approximator</strong></td>
<td style="text-align: left;"><strong>PQC</strong> <span class="arithmatex">\(U(\vec{\theta})\)</span></td>
<td style="text-align: left;">Exponential Hilbert space provides richer approximation via entanglement</td>
<td style="text-align: left;">Q-value or policy probability extracted via <strong>Pauli measurement</strong></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Gradient Calculation</strong></td>
<td style="text-align: left;"><strong>Parameter Shift Rule</strong></td>
<td style="text-align: left;">Analytically computes <span class="arithmatex">\(\nabla_{\vec{\theta}} Q\)</span> or <span class="arithmatex">\(\nabla_{\vec{\theta}} \pi\)</span> from measurements</td>
<td style="text-align: left;">Multiple circuit runs per parameter</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Action Generation</strong></td>
<td style="text-align: left;"><strong>Measurement-based Sampling</strong></td>
<td style="text-align: left;">Probability amplitudes sampled to yield probabilistic classical action <span class="arithmatex">\(a_t\)</span></td>
<td style="text-align: left;">Final measurement in computational basis</td>
</tr>
</tbody>
</table>
<h3 id="iii-exploration-and-architecture"><strong>III. Exploration and Architecture</strong></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">QRL Mechanism / Advantage</th>
<th style="text-align: left;">Classical Equivalent &amp; Difference</th>
<th style="text-align: left;">Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Exploration</strong></td>
<td style="text-align: left;"><strong>Coherent Exploration:</strong> Quantum Random Walks with interference-biased search</td>
<td style="text-align: left;">Classical <span class="arithmatex">\(\epsilon\)</span>-greedy: purely random (uncorrelated) noise</td>
<td style="text-align: left;">Navigating large state spaces; faster hitting times</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Amplitude Amplification</strong></td>
<td style="text-align: left;">Grover-style amplification increases probability of optimal actions</td>
<td style="text-align: left;">Classical exploration lacks targeted amplification</td>
<td style="text-align: left;">Accelerated convergence to optimal policy</td>
</tr>
<tr>
<td style="text-align: left;"><strong>QRL Agent Architecture</strong></td>
<td style="text-align: left;"><strong>Hybrid:</strong> PQC for policy/value, classical CPU for optimization, QRAM for memory</td>
<td style="text-align: left;">Classical: ANN for policy/value, RAM for memory</td>
<td style="text-align: left;"><strong>Adaptive quantum experiments:</strong> real-time control of physical quantum systems</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="references"><strong>References</strong></h2>
<hr />
<p>[1] Dunjko, V., Taylor, J. M., &amp; Briegel, H. J. (2016). "Quantum-enhanced machine learning." <em>Physical Review Letters</em>, 117(13), 130501.</p>
<p>[2] Dong, D., Chen, C., Li, H., &amp; Tarn, T. J. (2008). "Quantum reinforcement learning." <em>IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</em>, 38(5), 1207-1220.</p>
<p>[3] Lockwood, O., &amp; Si, M. (2020). "Reinforcement learning with quantum variational circuits." <em>Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</em>, 16(1), 245-251.</p>
<p>[4] Chen, S. Y. C., Yang, C. H. H., Qi, J., Chen, P. Y., Ma, X., &amp; Goan, H. S. (2020). "Variational quantum circuits for deep reinforcement learning." <em>IEEE Access</em>, 8, 141007-141024.</p>
<p>[5] Jerbi, S., Gyurik, C., Marshall, S., Briegel, H., &amp; Dunjko, V. (2021). "Variational quantum policies for reinforcement learning." <em>arXiv preprint arXiv:2103.05577</em>.</p>
<p>[6] Paparo, G. D., Dunjko, V., Makmal, A., Martin-Delgado, M. A., &amp; Briegel, H. J. (2014). "Quantum speedup for active learning agents." <em>Physical Review X</em>, 4(3), 031002.</p>
<p>[7] Skolik, A., Jerbi, S., &amp; Dunjko, V. (2022). "Quantum agents in the Gym: a variational quantum algorithm for deep Q-learning." <em>Quantum</em>, 6, 720.</p>
<p>[8] Saggio, V., Asenbeck, B. E., Hamann, A., Strmberg, T., Schiansky, P., Dunjko, V., ... &amp; Walther, P. (2021). "Experimental quantum speed-up in reinforcement learning agents." <em>Nature</em>, 591(7849), 229-233.</p>
<p>[9] Neukart, F., Von Dollen, D., Compostella, G., Seidel, C., Yarkoni, S., &amp; Parney, B. (2017). "Traffic flow optimization using a quantum annealer." <em>Frontiers in ICT</em>, 4, 29.</p>
<p>[10] Lamata, L. (2020). "Quantum reinforcement learning with quantum photonics." <em>Photonics</em>, 8(2), 33.</p>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../chapter-12/Chapter-12-Essay/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Chapter-12 Unsupervised Quantum Machine Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Chapter-12 Unsupervised Quantum Machine Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../../chapter-14/Chapter-14-Essay/" class="md-footer__link md-footer__link--next" aria-label="Next: Chapter-14 QUBO Family of Problems">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Chapter-14 QUBO Family of Problems
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://bigbookofcomputing.github.io" target="_blank" rel="noopener" title="bigbookofcomputing.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://x.com/bigbookofcomputing" target="_blank" rel="noopener" title="x.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.facebook.com/bigbookofcomputing" target="_blank" rel="noopener" title="www.facebook.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256c0 120 82.7 220.8 194.2 248.5V334.2h-52.8V256h52.8v-33.7c0-87.1 39.4-127.5 125-127.5 16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287v175.9C413.8 494.8 512 386.9 512 256"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/bigbookofcomputing" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.youtube.com/@big-book-of-computing" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
      <div class="md-consent" data-md-component="consent" id="__consent" hidden>
        <div class="md-consent__overlay"></div>
        <aside class="md-consent__inner">
          <form class="md-consent__form md-grid md-typeset" name="consent">
            


  


<h4>Cookie consent</h4>
<p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p>
<input class="md-toggle" type="checkbox" id="__settings" >
<div class="md-consent__settings">
  <ul class="task-list">
    
    
      
        
  
  
    
    
  
  <li class="task-list-item">
    <label class="task-list-control">
      <input type="checkbox" name="analytics" checked>
      <span class="task-list-indicator"></span>
      Google Analytics
    </label>
  </li>

      
    
    
      
        
  
  
    
    
  
  <li class="task-list-item">
    <label class="task-list-control">
      <input type="checkbox" name="github" checked>
      <span class="task-list-indicator"></span>
      GitHub
    </label>
  </li>

      
    
    
  </ul>
</div>
<div class="md-consent__controls">
  
    
      <button class="md-button md-button--primary">Accept</button>
    
    
    
  
    
    
    
      <label class="md-button" for="__settings">Manage settings</label>
    
  
</div>
          </form>
        </aside>
      </div>
      <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script>
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../static/mathjax-config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>